{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMQTKQPCHH9MYjzgJNlyiau"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This is notebook for quick Pandas Dataframe embeddings calculation using OpenAI's api.  \n",
        "It serves a practical role, because I'm calculating a lot of embeddings and it can take a long time.  \n",
        "\n",
        "I found it problematic that when just using \"apply\" function it takes ages, because each cell is a single call to API.  \n",
        "Another issue is that you usually need to be careful about sizes of texts.  \n",
        "And last issue is that running in colab/databricks, you have limited resources.  \n",
        "\n",
        "This notebook adresses all three, by:\n",
        "1. taking pandas rows in groups (this can be controlled with **df_batch_limit**)\n",
        "2. for each row in group it generates sliding windows (it's controlled  with **window_size** and **window_overlap**, be careful **window_size** cannot be larger than max API input size, the notebook assumes you know that)\n",
        "3. such sliced group is packed into batches that are sent to OpenAI's api (at this moment they support up to 1000 texts at once, you can change that with **api_batch_limit**)  \n",
        "   I need to mention: chunks for single row always fall into single batch, so there's small inefficiency and limitation, because batches are not always fully packed. It can be problem in 2 cases:\n",
        "   - if texts are very long, then the batches might have big \"holes\" at the end\n",
        "   - if any text (chunked into windows) is larger than **api_batch_limit**, then this notebook will simply not work\n",
        "4. results for those batches are then redistributed into particular rows (since each row was divided into windows, it will calculate mean(windows)/l2_norm"
      ],
      "metadata": {
        "id": "PHwxusqFmVXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets tiktoken"
      ],
      "metadata": {
        "id": "n_HTp5DP9DmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "from datasets import load_dataset\n",
        "from typing import Literal\n",
        "from scipy.spatial import distance"
      ],
      "metadata": {
        "id": "M8jffryZz3qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "X53kFrT3z62p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "\"\"\" Example usage\n",
        "response = client.embeddings.create(\n",
        "    input=[\"Your text string goes here\",\n",
        "           \"some other text\"],\n",
        "    model=\"text-embedding-3-small\"\n",
        ")\n",
        "\n",
        "print(response.data[0].embedding)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "O_VMm4hKzvOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"qgyd2021/e_commerce_customer_service\", \"product\")"
      ],
      "metadata": {
        "id": "RCEUk9-z4a_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load df\n",
        "df = ds['train'].to_pandas()\n",
        "# remove empty text ones\n",
        "df = df[(df['overview'] != '') & (df['overview'].notna())]"
      ],
      "metadata": {
        "id": "zy7iZS709Mya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# max length of single embedding input\n",
        "window_size = 512\n",
        "# how much each window overlaps previous\n",
        "window_overlap = 128\n",
        "window_step = window_size-window_overlap\n",
        "\n",
        "# max rows in a group to process together\n",
        "df_batch_limit = 100\n",
        "# max texts OpenAI can take\n",
        "api_batch_limit = 1000\n",
        "# column for which embeddings will be generated\n",
        "embedding_source_column = 'overview'"
      ],
      "metadata": {
        "id": "lTtTUPw-gjN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
      ],
      "metadata": {
        "id": "hvfwYb15iFm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Create a batch ID column\n",
        "df['batch_id'] = np.arange(len(df)) // df_batch_limit\n",
        "\n",
        "# Group by batch ID and apply your function to each group\n",
        "def process_group(group):\n",
        "    # Extract data from group\n",
        "    source_texts = group[embedding_source_column].tolist()\n",
        "\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    texts_tokens = [tokenizer.encode(text) for text in source_texts]\n",
        "\n",
        "\n",
        "    slices = [[tokenizer.decode(tokens[i*window_step:i*window_step+window_size]) for i in range(int(np.ceil(len(tokens)/window_step)))] for tokens in texts_tokens]\n",
        "    slices_lengths = [len(row_slices) for row_slices in slices]\n",
        "    if any(x > api_batch_limit for x in slices_lengths):\n",
        "        raise ValueError(f\"Single row/text exceeds api batch limit of {api_batch_limit}\")\n",
        "\n",
        "    def acc_divide_slices(slices_lengths, slices, result_boxes=[]):\n",
        "        if len(slices_lengths) == 0:\n",
        "            return result_boxes\n",
        "        else:\n",
        "            if len(result_boxes) == 0:\n",
        "                result_boxes.append({'total_size': slices_lengths[0],\n",
        "                                     'slice_sizes': [slices_lengths[0]],\n",
        "                                     'api_package': slices[0]\n",
        "                                     })\n",
        "            elif (result_boxes[-1]['total_size'] + slices_lengths[0]) <= api_batch_limit:\n",
        "                result_boxes[-1]['total_size'] = (result_boxes[-1]['total_size'] + slices_lengths[0])\n",
        "                result_boxes[-1]['slice_sizes'].append(slices_lengths[0])\n",
        "                result_boxes[-1]['api_package'].extend(slices[0])\n",
        "            else:\n",
        "                result_boxes.append({'total_size': slices_lengths[0],\n",
        "                                     'slice_sizes': [slices_lengths[0]],\n",
        "                                     'api_package': slices[0]\n",
        "                                     })\n",
        "\n",
        "            if len(slices_lengths) == 1:\n",
        "                return result_boxes\n",
        "            return acc_divide_slices(slices_lengths[1:], slices[1:], result_boxes)\n",
        "\n",
        "    prepared_packages = acc_divide_slices(slices_lengths, slices)\n",
        "\n",
        "    packages_with_results = [\n",
        "        {\n",
        "            'total_size': package['total_size'],\n",
        "            'slice_sizes': package['slice_sizes'],\n",
        "            'api_package': package['api_package'],\n",
        "            'api_results': [\n",
        "                item.embedding for item in client.embeddings.create(\n",
        "                    input=package['api_package'],\n",
        "                    model=\"text-embedding-3-small\"\n",
        "                    ).data\n",
        "                ]\n",
        "            }\n",
        "        for package in prepared_packages]\n",
        "\n",
        "\n",
        "    def calc_results(prepared_package: list[dict], combination_method: Literal['mean', 'max']) -> list:\n",
        "        res = []\n",
        "        for element in prepared_package:\n",
        "            slice_sizes = element['slice_sizes']\n",
        "            so_far_popped = 0\n",
        "            for row_id in range(len(slice_sizes)):\n",
        "                row_res_unnormalized = np.mean(np.array(element['api_results'][so_far_popped:so_far_popped+slice_sizes[row_id]]), axis=0)\n",
        "                norm = np.linalg.norm(row_res_unnormalized)\n",
        "                row_res = row_res_unnormalized/norm\n",
        "                so_far_popped = so_far_popped+slice_sizes[row_id]\n",
        "                res.append(row_res)\n",
        "            assert so_far_popped == element['total_size']\n",
        "\n",
        "\n",
        "        return res\n",
        "\n",
        "    results = calc_results(packages_with_results, 'mean')\n",
        "\n",
        "    # Return a Series with the results\n",
        "    return pd.Series(results, index=group.index)\n",
        "\n",
        "# Apply the function to each group and assign to new column\n",
        "df[f'{embedding_source_column}_embeddings'] = df.groupby('batch_id').apply(process_group).reset_index(level=0, drop=True)\n",
        "# Remove the temporary batch_id column if desired\n",
        "df.drop('batch_id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "cheaFyBMEzo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.iloc[0]['overview_embeddings'])\n",
        "print(df.iloc[1]['overview_embeddings'])\n",
        "print(1-distance.cosine(df.iloc[0]['overview_embeddings'], df.iloc[1]['overview_embeddings']))"
      ],
      "metadata": {
        "id": "rw36hccdFFWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i in range(100):\n",
        "    print(np.linalg.norm(df.iloc[i]['overview_embeddings']))"
      ],
      "metadata": {
        "id": "l_89LHcBiKw0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}